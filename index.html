<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LiveAvatar Client - SnapLogic Integrated</title>
    <script src="https://cdn.jsdelivr.net/npm/livekit-client/dist/livekit-client.umd.min.js"></script>
    <style>
        body { 
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; 
            display: flex; 
            flex-direction: column; 
            align-items: center; 
            padding: 40px; 
            background-color: #f5f5f7;
        }
        h1 { margin-bottom: 20px; color: #333; }
        
        #video-container { 
            width: 100%; 
            max-width: 800px; 
            aspect-ratio: 16/9; 
            background: #000; 
            margin-top: 20px; 
            position: relative; 
            border-radius: 12px;
            overflow: hidden;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        video, audio { width: 100%; height: 100%; object-fit: cover; }

        .controls { 
            background: white; padding: 20px; border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05); display: flex; gap: 10px; align-items: center;
        }
        
        button { 
            padding: 10px 20px; cursor: pointer; background-color: #0070f3; color: white;
            border: none; border-radius: 4px; font-weight: 600;
        }
        button:disabled { background-color: #ccc; cursor: not-allowed; }
        button#disconnectBtn { background-color: #e00; }
        
        button.active-mic { background-color: #e00; }
        
        #status { 
            margin-top: 15px; color: #666; font-family: monospace;
            background: #eee; padding: 8px 12px; border-radius: 4px;
        }

        /* --- NEW STYLES FOR TRANSCRIPT --- */
        .transcript-container {
            width: 100%;
            max-width: 800px;
            margin-top: 20px;
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
            display: flex;
            flex-direction: column;
            gap: 10px;
        }

        .transcript-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .transcript-header h3 { margin: 0; color: #333; }

        textarea#transcriptLog {
            width: 100%;
            height: 200px;
            padding: 15px;
            border: 1px solid #ddd;
            border-radius: 6px;
            font-family: monospace;
            font-size: 14px;
            line-height: 1.5;
            resize: vertical;
            background-color: #fafafa;
            box-sizing: border-box; /* Ensures padding doesn't break width */
        }

        button.download-btn {
            background-color: #28a745;
            align-self: flex-end;
        }
    </style>
</head>
<body>

    <h1>LiveAvatar Client</h1>
    
    <div class="controls">
        <button onclick="initSession()">Start & Connect</button>
        <button onclick="toggleMic()" disabled id="micBtn">Turn On Mic</button>
        <button onclick="disconnect()" disabled id="disconnectBtn">Disconnect</button>
    </div>

    <div id="status">Ready to connect...</div>

    <div id="video-container"></div>

    <div class="transcript-container">
        <div class="transcript-header">
            <h3>Live Transcript</h3>
            <button onclick="downloadTranscript()" class="download-btn">Download (.txt)</button>
        </div>
        <textarea id="transcriptLog" readonly placeholder="Conversation will appear here..."></textarea>
    </div>

    <script>
        let currentRoom = null;
        let recognition = null; // For local speech recognition
        const SNAPLOGIC_URL = "https://tpalpb1wva002.verizon.com:8888/api/1/rest/feed/run/task/VerizonEISDev/DeliveryAssurance/shared/Embedded_Agent_Token_Request%20Task_Gnd?bearer_token=8tn6rpItp0F8RIYNNGq2LECBvdi9is2X";

        // --- TRANSCRIPT FUNCTIONS ---

        function appendToTranscript(sender, text) {
            const log = document.getElementById('transcriptLog');
            const timestamp = new Date().toLocaleTimeString();
            const entry = `[${timestamp}] ${sender}: ${text}\n`;
            log.value += entry;
            log.scrollTop = log.scrollHeight; // Auto-scroll to bottom
        }

        function downloadTranscript() {
            const content = document.getElementById('transcriptLog').value;
            if (!content) {
                alert("No transcript to download yet.");
                return;
            }
            const blob = new Blob([content], { type: 'text/plain' });
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `transcript-${new Date().toISOString().slice(0,19).replace(/:/g,"-")}.txt`;
            document.body.appendChild(a);
            a.click();
            document.body.removeChild(a);
            window.URL.revokeObjectURL(url);
        }

        // Initialize Web Speech API for local user transcription
        function initSpeechRecognition() {
            if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                recognition = new SpeechRecognition();
                recognition.continuous = true;
                recognition.interimResults = false;
                recognition.lang = 'en-US';

                recognition.onresult = (event) => {
                    const last = event.results.length - 1;
                    const text = event.results[last][0].transcript;
                    appendToTranscript("You", text);
                };

                recognition.onerror = (event) => {
                    console.error("Speech Recognition Error:", event.error);
                };
            } else {
                console.warn("Web Speech API not supported in this browser.");
                appendToTranscript("System", "Local speech-to-text not supported in this browser.");
            }
        }

        // --- CORE FUNCTIONS ---

        function updateStatus(msg) {
            const statusEl = document.getElementById('status');
            if(statusEl) statusEl.innerText = msg;
            console.log(`[Status] ${msg}`);
        }

        async function initSession() {
            updateStatus("Requesting Token from SnapLogic...");
            // Clear transcript on new session
            document.getElementById('transcriptLog').value = ""; 

            try {
                // 1. Fetch Token from SnapLogic
                const slResponse = await fetch(SNAPLOGIC_URL);
                if (!slResponse.ok) throw new Error("SnapLogic Fetch Error: " + slResponse.status);
                const slData = await slResponse.json();
                
                let sessionToken = null;
                if (Array.isArray(slData) && slData.length > 0 && slData[0].session_token) {
                    sessionToken = slData[0].session_token;
                } else {
                    throw new Error("SnapLogic response format incorrect.");
                }

                updateStatus("Token received. Initializing Session...");

                // 2. Call API to get LiveKit credentials
                const response = await fetch('https://api.liveavatar.com/v1/sessions/start', {
                    method: 'POST',
                    headers: { 'Authorization': 'Bearer ' + sessionToken, 'Accept': 'application/json' }
                });

                if (!response.ok) throw new Error("Avatar API Error: " + response.status);

                const data = await response.json();
                const innerData = data.data || data; 
                const liveKitUrl = innerData.livekit_url;
                const liveKitToken = innerData.livekit_client_token;

                if (!liveKitUrl || !liveKitToken) throw new Error("Missing keys in response.");

                // 3. Connect to LiveKit
                await connectToLiveKit(liveKitUrl, liveKitToken);

                // Initialize Speech Recognition logic
                initSpeechRecognition();

            } catch (error) {
                updateStatus("Error: " + error.message);
                console.error(error);
            }
        }

        async function connectToLiveKit(url, token) {
            updateStatus("Connecting to Room...");
            try {
                currentRoom = new LivekitClient.Room({ adaptiveStream: true, dynacast: true });

                // Video Handling
                currentRoom.on(LivekitClient.RoomEvent.TrackSubscribed, (track) => {
                    const element = track.attach();
                    document.getElementById('video-container').appendChild(element);
                });

                // --- DATA HANDLING (Agent Text) ---
                currentRoom.on(LivekitClient.RoomEvent.DataReceived, (payload, participant) => {
                    const decoder = new TextDecoder();
                    const strData = decoder.decode(payload);
                    
                    // Attempt to parse JSON (common in LiveKit agents) or use raw string
                    let displayText = strData;
                    try {
                        const jsonData = JSON.parse(strData);
                        if(jsonData.message) displayText = jsonData.message;
                        else if(jsonData.text) displayText = jsonData.text;
                    } catch(e) {
                        // Not JSON, use raw string
                    }

                    const senderName = participant ? "Avatar" : "System";
                    appendToTranscript(senderName, displayText);
                });
                
                currentRoom.on(LivekitClient.RoomEvent.Disconnected, () => {
                    updateStatus("Disconnected.");
                    resetUI();
                });

                await currentRoom.connect(url, token);
                updateStatus("Connected! Video starting...");
                
                document.getElementById('disconnectBtn').disabled = false;
                document.getElementById('micBtn').disabled = false;

            } catch (error) {
                updateStatus("Connection Failed: " + error.message);
                console.error(error);
            }
        }

        async function toggleMic() {
            if (!currentRoom) return;

            const btn = document.getElementById('micBtn');
            const isEnabled = currentRoom.localParticipant.isMicrophoneEnabled;

            try {
                await currentRoom.localParticipant.setMicrophoneEnabled(!isEnabled);
                
                if (!isEnabled) {
                    // Mic is now ON
                    btn.innerText = "Mute Mic";
                    btn.classList.add("active-mic");
                    updateStatus("Microphone live!");
                    
                    // Start local transcription
                    if(recognition) recognition.start();
                } else {
                    // Mic is now OFF
                    btn.innerText = "Turn On Mic";
                    btn.classList.remove("active-mic");
                    updateStatus("Microphone muted.");

                    // Stop local transcription
                    if(recognition) recognition.stop();
                }
            } catch (error) {
                console.error(error);
                updateStatus("Mic Error: Check browser permissions.");
            }
        }

        async function disconnect() {
            if (currentRoom) {
                await currentRoom.disconnect();
            }
            if (recognition) {
                recognition.stop();
            }
        }

        function resetUI() {
            document.getElementById('video-container').innerHTML = '';
            document.getElementById('disconnectBtn').disabled = true;
            
            const micBtn = document.getElementById('micBtn');
            micBtn.disabled = true;
            micBtn.innerText = "Turn On Mic";
            micBtn.classList.remove("active-mic");
            
            currentRoom = null;
        }
    </script>
</body>
</html>

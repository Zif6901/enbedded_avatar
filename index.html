<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Device Showcase Virtual Coworker (Debug Mode)</title>
    <script src="https://cdn.jsdelivr.net/npm/livekit-client/dist/livekit-client.umd.min.js"></script>
    <style>
        body { 
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; 
            display: flex; 
            flex-direction: column; 
            align-items: center; 
            padding: 40px; 
            background-color: #f5f5f7;
        }
        h1 { margin-bottom: 20px; color: #333; }
        
        #processing-bar {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 6px; 
            background: linear-gradient(90deg, #0070f3, #00c6ff);
            z-index: 1000;
            display: none; 
            animation: pulse 1.5s infinite;
        }
        @keyframes pulse {
            0% { opacity: 0.5; }
            50% { opacity: 1; }
            100% { opacity: 0.5; }
        }
        #processing-text {
            position: fixed;
            top: 15px;
            background: rgba(0, 112, 243, 0.9);
            color: white;
            padding: 8px 20px;
            border-radius: 20px;
            font-size: 14px;
            font-weight: bold;
            z-index: 1001;
            display: none;
            box-shadow: 0 2px 5px rgba(0,0,0,0.2);
        }

        #video-container { 
            width: 100%; 
            max-width: 800px; 
            aspect-ratio: 16/9; 
            background: #000; 
            margin-top: 20px; 
            position: relative; 
            border-radius: 12px;
            overflow: hidden;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        video, audio { width: 100%; height: 100%; object-fit: cover; }

        .controls { 
            background: white; padding: 20px; border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05); display: flex; gap: 10px; align-items: center;
        }
        
        button { 
            padding: 10px 20px; cursor: pointer; background-color: #0070f3; color: white;
            border: none; border-radius: 4px; font-weight: 600;
        }
        button:disabled { background-color: #ccc; cursor: not-allowed; }
        button#disconnectBtn { background-color: #e00; }
        button.active-mic { background-color: #e00; }
        
        #status { 
            margin-top: 15px; color: #666; font-family: monospace;
            background: #eee; padding: 8px 12px; border-radius: 4px;
        }

        .transcript-container {
            width: 100%;
            max-width: 800px;
            margin-top: 20px;
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
            display: flex;
            flex-direction: column;
            gap: 10px;
        }

        .transcript-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        textarea#transcriptLog {
            width: 100%;
            height: 300px; /* Made taller for logs */
            padding: 15px;
            border: 1px solid #ddd;
            border-radius: 6px;
            font-family: monospace;
            font-size: 12px; /* Smaller font for logs */
            line-height: 1.5;
            resize: vertical;
            background-color: #1e1e1e; /* Dark mode for logs */
            color: #0f0; /* Green text */
            white-space: pre-wrap; 
        }

        .input-group {
            display: flex;
            gap: 10px;
            margin-top: 10px;
            border-top: 1px solid #eee;
            padding-top: 15px;
        }
        
        input#textCommand {
            flex-grow: 1;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 4px;
            font-size: 14px;
        }
    </style>
</head>
<body>

    <div id="processing-bar"></div>
    <div id="processing-text">Processing...</div>

    <h1>Device Showcase Virtual Coworker</h1>
    
    <div class="controls">
        <button onclick="initSession()">Start & Connect</button>
        <button onclick="toggleMic()" disabled id="micBtn">Mic On/Off</button>
        <button onclick="disconnect()" disabled id="disconnectBtn">Disconnect</button>
    </div>

    <div id="status">Ready to connect...</div>

    <div id="video-container"></div>

    <div class="transcript-container">
        <div class="transcript-header">
            <h3>Debug Log & Transcript</h3>
            <button onclick="downloadTranscript()" style="background-color:#28a745; padding:8px 16px; font-size:14px;">Download Log</button>
        </div>
        <textarea id="transcriptLog" readonly placeholder="System logs will appear here..."></textarea>
        
        <div class="input-group">
            <input type="text" id="textCommand" placeholder="Type a command..." onkeypress="handleInputKey(event)">
            <button onclick="sendTextCommand()" id="sendBtn" disabled>Send</button>
        </div>
    </div>

    <script>
        let currentRoom = null;
        let recognition = null; 
        let processingTimeout = null;
        let agentAudioInterval = null;
        let audioInterval = null;
        let isSpeaking = false;
        let silenceTimer = null;
        const SILENCE_THRESHOLD = 0.01; 
        const SILENCE_DELAY_MS = 1000; 
        
        const SNAPLOGIC_URL = "https://tpalpb1wva002.verizon.com:8888/api/1/rest/feed/run/task/VerizonEISDev/DeliveryAssurance/shared/Embedded_Agent_Token_Request%20Task_Gnd?bearer_token=8tn6rpItp0F8RIYNNGq2LECBvdi9is2X";

        // --- UI HELPERS ---
        function setProcessing(isProcessing) {
            const bar = document.getElementById('processing-bar');
            const txt = document.getElementById('processing-text');
            
            if (processingTimeout) clearTimeout(processingTimeout);

            if (isProcessing) {
                bar.style.display = 'block';
                txt.style.display = 'block';
                updateStatus("Processing user input...");
                
                // 15s Safety Timeout
                processingTimeout = setTimeout(() => {
                    if (bar.style.display === 'block') {
                        setProcessing(false);
                        logToConsole("[WARN] Processing timed out. No response detected.");
                        updateStatus("Connected (No response detected)");
                    }
                }, 15000);

            } else {
                bar.style.display = 'none';
                txt.style.display = 'none';
                updateStatus("Connected & Listening");
            }
        }

        // --- ENHANCED LOGGING FUNCTION ---
        function logToConsole(message) {
            const log = document.getElementById('transcriptLog');
            const timestamp = new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit', second: '2-digit', fraction: '3-digit' });
            log.value += `[${timestamp}] ${message}\n`;
            log.scrollTop = log.scrollHeight; 
            console.log(message); // Also log to browser dev tools
        }

        function downloadTranscript() {
            const content = document.getElementById('transcriptLog').value;
            if (!content) return alert("No log to download.");
            const blob = new Blob([content], { type: 'text/plain' });
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `debug_log.txt`;
            a.click();
        }

        function initSpeechRecognition() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (SpeechRecognition) {
                recognition = new SpeechRecognition();
                recognition.continuous = true;      
                recognition.interimResults = false; 
                recognition.lang = 'en-US';
                recognition.onresult = (event) => {
                    const last = event.results.length - 1;
                    const text = event.results[last][0].transcript;
                    logToConsole(`[USER-SPEECH] Detected: "${text}"`);
                };
            }
        }

        // --- MAIN CONNECTION ---
        async function initSession() {
            updateStatus("Requesting Token...");
            document.getElementById('transcriptLog').value = ""; 
            logToConsole("[SYSTEM] Starting Session Init...");

            try {
                // Audio Context Resume
                const AudioContext = window.AudioContext || window.webkitAudioContext;
                if (AudioContext) {
                    const ctx = new AudioContext();
                    await ctx.resume();
                    ctx.close();
                    logToConsole("[SYSTEM] AudioContext Resumed");
                }

                const slResponse = await fetch(SNAPLOGIC_URL);
                if (!slResponse.ok) throw new Error("SnapLogic Error: " + slResponse.status);
                const slData = await slResponse.json();
                const sessionToken = slData[0]?.session_token;
                
                if (!sessionToken) {
                     logToConsole("[ERROR] No session token in SnapLogic response");
                     throw new Error("No session token.");
                }
                logToConsole("[SYSTEM] Token Received. Calling Avatar API...");

                const response = await fetch('https://api.liveavatar.com/v1/sessions/start', {
                    method: 'POST',
                    headers: { 'Authorization': 'Bearer ' + sessionToken, 'Accept': 'application/json' }
                });

                if (!response.ok) {
                    logToConsole(`[ERROR] Avatar API Failed: ${response.status}`);
                    throw new Error("Avatar API Error: " + response.status);
                }
                const data = await response.json();
                const innerData = data.data || data; 
                logToConsole("[SYSTEM] Avatar Session Started. Connecting to LiveKit...");
                
                await connectToLiveKit(innerData.livekit_url, innerData.livekit_client_token);
                initSpeechRecognition();

            } catch (error) {
                logToConsole(`[ERROR] Init Failed: ${error.message}`);
                updateStatus("Error: " + error.message);
            }
        }

        async function connectToLiveKit(url, token) {
            updateStatus("Connecting to Room...");
            try {
                currentRoom = new LivekitClient.Room({ adaptiveStream: true, dynacast: true });

                // 1. MONITOR AGENT AUDIO
                currentRoom.on(LivekitClient.RoomEvent.TrackSubscribed, (track, publication, participant) => {
                    document.getElementById('video-container').appendChild(track.attach());
                    logToConsole(`[ROOM] Track Subscribed: ${track.kind} (Source: ${track.source})`);
                    
                    if (track.kind === LivekitClient.Track.Kind.Audio) {
                        const element = track.attach();
                        element.onplay = () => {
                            logToConsole("[AUDIO] Agent audio started playing.");
                            setProcessing(false);
                        };
                        startAgentAudioMonitoring(participant);
                    }
                });

                // 2. DATA HANDLING (VERBOSE LOGGING)
                currentRoom.on(LivekitClient.RoomEvent.DataReceived, (payload, participant, kind, topic) => {
                    const decoder = new TextDecoder();
                    const strData = decoder.decode(payload);
                    
                    // Log Raw Data immediately
                    logToConsole(`[RECEIVED] Topic: "${topic || 'default'}" | Data: ${strData}`);
                    
                    try {
                        const jsonData = JSON.parse(strData);
                        
                        // Universal Reset on any agent activity
                        if (jsonData.event_type && jsonData.event_type.startsWith('agent')) {
                            setProcessing(false);
                        }

                        // Determine Speaker Label
                        let senderName = "AI Eddie"; 
                        if (jsonData.role === 'user' || jsonData.type === 'transcription' || jsonData.type === 'transcript') {
                            senderName = "User";
                        }

                        // Log parsed message if available
                        let messageText = jsonData.text || jsonData.message || jsonData.transcript;
                        if (messageText) {
                            // Don't double log if we already logged raw
                            // Just ensure UI updates
                        }
                    } catch(e) {
                        // Not JSON, that's fine, we already logged raw
                    }
                });
                
                // Log Connection State Changes
                currentRoom.on(LivekitClient.RoomEvent.Connected, () => logToConsole("[ROOM] Connected to LiveKit Server"));
                currentRoom.on(LivekitClient.RoomEvent.Disconnected, () => logToConsole("[ROOM] Disconnected"));
                currentRoom.on(LivekitClient.RoomEvent.Reconnecting, () => logToConsole("[ROOM] Reconnecting..."));
                currentRoom.on(LivekitClient.RoomEvent.Reconnected, () => logToConsole("[ROOM] Reconnected"));

                await currentRoom.connect(url, token);
                
                document.getElementById('disconnectBtn').disabled = false;
                document.getElementById('micBtn').disabled = false;
                document.getElementById('sendBtn').disabled = false;

                try {
                    await currentRoom.localParticipant.setMicrophoneEnabled(true);
                    document.getElementById('micBtn').classList.add("active-mic");
                    startUserAudioMonitoring(); 
                    if (recognition) recognition.start();
                    updateStatus("Connected & Mic Live");
                } catch(e) {
                    logToConsole(`[WARN] Mic start failed: ${e.message}`);
                    updateStatus("Connected (Mic manual start required)");
                }

            } catch (error) {
                logToConsole(`[ERROR] LiveKit Connection Failed: ${error.message}`);
                updateStatus("Connection Failed: " + error.message);
            }
        }

        // --- AGENT VOLUME MONITORING ---
        function startAgentAudioMonitoring(participant) {
            if (agentAudioInterval) clearInterval(agentAudioInterval);
            agentAudioInterval = setInterval(() => {
                if (!participant) return;
                if (participant.audioLevel > 0.01) {
                    // Agent is speaking!
                    setProcessing(false);
                }
            }, 100);
        }

        // --- USER VAD ---
        function startUserAudioMonitoring() {
            if (audioInterval) clearInterval(audioInterval);
            audioInterval = setInterval(() => {
                if (!currentRoom || !currentRoom.localParticipant) return;
                const audioLevel = currentRoom.localParticipant.audioLevel;

                if (audioLevel > SILENCE_THRESHOLD) {
                    if (!isSpeaking) {
                        isSpeaking = true;
                        document.getElementById('processing-bar').style.display = 'none';
                        document.getElementById('processing-text').style.display = 'none';
                        updateStatus("Listening...");
                        // Only log if silence duration was significant to avoid spam
                        if (!silenceTimer) logToConsole("[VAD] User Speaking...");
                    }
                    if (silenceTimer) clearTimeout(silenceTimer);
                    silenceTimer = null;
                } else if (isSpeaking) {
                    if (!silenceTimer) {
                        silenceTimer = setTimeout(() => {
                            isSpeaking = false;
                            setProcessing(true); 
                            logToConsole("[VAD] User Silence Detected -> Processing");
                        }, SILENCE_DELAY_MS);
                    }
                }
            }, 50); 
        }

        async function toggleMic() {
            if (!currentRoom) return;
            const btn = document.getElementById('micBtn');
            const isEnabled = currentRoom.localParticipant.isMicrophoneEnabled;
            await currentRoom.localParticipant.setMicrophoneEnabled(!isEnabled);
            if (!isEnabled) {
                btn.classList.add("active-mic");
                startUserAudioMonitoring();
                updateStatus("Mic Live");
                logToConsole("[USER] Mic Unmuted");
            } else {
                btn.classList.remove("active-mic");
                if (audioInterval) clearInterval(audioInterval);
                updateStatus("Mic Muted");
                logToConsole("[USER] Mic Muted");
            }
        }

        function handleInputKey(event) {
            if (event.key === "Enter") sendTextCommand();
        }

        // --- COMMAND SENDING (WITH LOGGING) ---
        async function sendTextCommand() {
            const inputField = document.getElementById('textCommand');
            const text = inputField.value.trim();
            if (!text || !currentRoom) return;

            setProcessing(true); 
            logToConsole(`[USER-INPUT] Sending: "${text}"`);

            const wasMicEnabled = currentRoom.localParticipant.isMicrophoneEnabled;
            if (wasMicEnabled) {
                await currentRoom.localParticipant.setMicrophoneEnabled(false);
                logToConsole("[SYSTEM] Mic auto-muted for text command");
            }

            try {
                const encoder = new TextEncoder();
                const timestamp = Date.now();

                // 1. "Speak" Command
                const payloadSpeak = JSON.stringify({ type: "speak", text: text });

                // 2. Chat Payload
                const payloadChat = JSON.stringify({ message: text, timestamp: timestamp });

                // 3. Conversation Input
                const payloadInput = JSON.stringify({ type: "conversation.input", text: text });

                // --- BROADCAST & LOG ---
                
                // A. Send 'speak' (Reliable)
                await currentRoom.localParticipant.publishData(encoder.encode(payloadSpeak), { reliable: true });
                logToConsole(`[SENT] Topic: (default) | Data: ${payloadSpeak}`);

                // B. Send 'chat' (lk-chat-topic)
                await currentRoom.localParticipant.publishData(encoder.encode(payloadChat), { reliable: true, topic: "lk-chat-topic" });
                logToConsole(`[SENT] Topic: lk-chat-topic | Data: ${payloadChat}`);

                // C. Send 'conversation.input' (Reliable)
                await currentRoom.localParticipant.publishData(encoder.encode(payloadInput), { reliable: true });
                logToConsole(`[SENT] Topic: (default) | Data: ${payloadInput}`);

                inputField.value = "";
                
            } catch (e) {
                logToConsole(`[ERROR] Send Failed: ${e.message}`);
                setProcessing(false);
                updateStatus("Send Failed");
            } finally {
                if (wasMicEnabled) {
                    setTimeout(async () => {
                        try {
                            await currentRoom.localParticipant.setMicrophoneEnabled(true);
                            logToConsole("[SYSTEM] Mic auto-restored");
                        } catch(e) {}
                    }, 2000); 
                }
            }
        }

        async function disconnect() {
            logToConsole("[USER] Disconnecting...");
            if (currentRoom) await currentRoom.disconnect();
            if (audioInterval) clearInterval(audioInterval);
            if (agentAudioInterval) clearInterval(agentAudioInterval);
            setProcessing(false);
            resetUI();
        }

        function updateStatus(msg) {
            document.getElementById('status').innerText = msg;
        }

        function resetUI() {
            document.getElementById('video-container').innerHTML = '';
            document.getElementById('disconnectBtn').disabled = true;
            document.getElementById('sendBtn').disabled = true;
            document.getElementById('micBtn').classList.remove("active-mic");
            document.getElementById('micBtn').disabled = true;
            currentRoom = null;
            logToConsole("[SYSTEM] UI Reset");
        }
    </script>
</body>
</html>
